Apache Kafka :-  Pull Mechanism.

There are 3 types of Pipelines :- 
a.) Batch Data Pipeline  :- Data needs to be computed in the batches or some kind of historical data.
Eg. Monthly Credit Card Analysis, electricity Bills. 
Frequency/Threshold (Daily/Weekly/Monthly/Yearly).
Tools :- Airflow,Hive, Spark , Data Warehouse, Visualization.

b.) Near  Real Time Pipeline :- We process the data in micro batches. Eg. 

Tool: Spark Structured Streaming.

c.) Real Time Pipeline :- As soon as any event happens, the data generated is processed in fraction of seconds. 
Tool:- Database CDC (Change Data Capture). we need to capture the change as soon as data is changed in Database. 
Live Leaderboard. board is updated in real time. Twiteer, Bank Transaction, fraud detection (DE + DS), cricket score board. 

Tool :- Apache Flink


In order to build, real time data pipelines, we need messaging queue kind of frameworks so that data can be holded for some time. 

Apache Kafka + confluent kafka (same) is messaging queue. SQS (Simple Queue Service)(AWS). 

Apache kafka is open source. Not is confluent kafka. 
kafka is created at linkedIn. 

                                                              Restaurant (Cassandra)
eg.  Food Delivery                                            /\      |
                                                          / ---|      |
                              consumer                 /              |
                                   |                /                 |
Food app                    |           |         /                  \|/
source --------x-------------- Process ------                     Batch Processing
                                                              (Spark (Structured Streaming))
    |                       |     /\    |                               |
   \|/                            |                                     |
 Queue(Kafka)                     |                                    \|/
 Stores all the messages  --------                                      DWH (Transformed Data)
                            
                            
Apache Flink :- pure real time. it is horizontally scalable. processing  engine like spark.
can take any source like DB, Non-Structural data, stores and processes. 

Kakfa Components :- 
Producer - Application which publishes Data In kafka. 
Consumer - application which consumes from kafka.
Cluster - Kafka is a distributed System.
Broker -  Nodes in Kafka Cluster where actual data gets stored. 

Topic -  small segments or space given with in kafka to store the data. 

Partitions -  Dividing the data in multiple parts inside a topic. each topic will be divided 
into multiple partitions.  in a partition, messages will maintain their arrival sequence based on the 
timestamp.
On topic level, sequencing is not available.

Replicas -   if the replica factor is 3, there will be 3 broker each holding a replica of partitions.

Offsets - Unique Id number for a  message with in a partition. 
Eg. P1 = []  # empty partition P1
after data is added 
P1 = [Mo,M1......]
      0   1   2   3   <-  Offsets
  this means Mo has been published to offset [0]
  

Commits -  it is very important. Mechanism in kafka to maintain or keep a track of messages consumed from kafka topic.

Synchronous Commits :- Blocking Call. if response is not there, it will not proceed further to 
      next steps. if it fails, then it will start from Step -1 
   Scenaio :- whereever we are concerned about data consitency then we use this. 
   Impact - Will be having high latency.
Asynchronous Commits :- response May Come later. if response is not there from commit, it will process further to next steps not waiting for the response. (Fire and Forget).
Scenarios :- Data inconsistency is tolerated. 
Impact - Low latency

Eg. Topic (Order_Data) -> Kafka will maintain some information under Some Topic and it will start storing the updated offset from which data was consumed and consumer application made a successfull commits.
after first read, commit will happen and offset 0 will be maintained in the metadata. 
In 2nd read, data will be consumed from offset 1.

Pallelism in Kafka - Multi threaded - Producer -> Kafka Topic with Partition <- Single Consumer (Round robin Distribution) (Pushing back to queue). 

a. Production speed is higher than consumption. 
in this case, back pressue will be hit as consumer will not be able to match with producer. 

to solve this, we should scale our application. So multi threaded consumer set will be created which will handle the load from producer - kafka queue.


So for Parallelism :-  from 1 partition, only 1 consumer of the same group will be able to read the data. there is no mix between consumer group n kafka topic partitions.

eg. P1 and P2 will be processed by C1 and P3 and P4 will be processed by C2.


Consumer group - all the consumers should have the same consumer group ID(Custom Name).
2 different group id consumers can read from same partition.

we should increase the number of the partitions.

Back Pressure - consumer to producer
                            
Kafka Model -  pub/sub

Kafka Architecture :- 
it is basically Master - Slave Architecture. 

                [ Leader ]  -> Manage
            /       |           \
         Broker    Broker       broker 
         
         s1       s2           s3
         
Key - Value (should be carefull about choosing the key column).
Does Data Skewness problem exist in Kafka ? I think Yes.  as we publish data in key-value form.

Challenges :- 
duplicate partition processing/ partial processing

we can do state maintenance / meta data to handle this scenarios.

Schema Registry :- it provides a centralized repository for managing and validating Schema
for topic message data, and for serialization and deserialization of the data over the network.

Producers and consumers to kafka topics can use schemas to ensure data consistency and compatilibility
as schema evolve.


serialization - Deserialization. 

serialization is the process of converting structured data into raw form. (eg.  Object -> ByteForm)

Deserialization is the process of  reconstructing structured forms from the data's bit stream form. 




